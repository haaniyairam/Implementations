{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOJqUN/DkQjRGK7rhMDajZW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UKc_6682EgSU","executionInfo":{"status":"ok","timestamp":1720767453563,"user_tz":-330,"elapsed":9660,"user":{"displayName":"Haaniya Iram","userId":"01967222098810418992"}},"outputId":"060242ec-fdda-44cf-feca-833f149d5c62"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"]}],"source":["!pip install matplotlib"]},{"cell_type":"code","source":["import gym\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import deque\n","\n","# Define the Q-network\n","class QNetwork(nn.Module):\n","    def __init__(self, state_size, action_size):\n","        super(QNetwork, self).__init__()\n","        self.fc1 = nn.Linear(state_size, 64)\n","        self.fc2 = nn.Linear(64, 64)\n","        self.fc3 = nn.Linear(64, action_size)\n","\n","    def forward(self, state):\n","        x = torch.relu(self.fc1(state))\n","        x = torch.relu(self.fc2(x))\n","        return self.fc3(x)\n","\n","# Define the replay buffer\n","class ReplayBuffer:\n","    def __init__(self, buffer_size, batch_size):\n","        self.memory = deque(maxlen=buffer_size)\n","        self.batch_size = batch_size\n","\n","    def add(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def sample(self):\n","        experiences = random.sample(self.memory, k=self.batch_size)\n","        states, actions, rewards, next_states, dones = zip(*experiences)\n","        return (\n","            torch.tensor(states, dtype=torch.float32),\n","            torch.tensor(actions, dtype=torch.int64),\n","            torch.tensor(rewards, dtype=torch.float32),\n","            torch.tensor(next_states, dtype=torch.float32),\n","            torch.tensor(dones, dtype=torch.uint8)\n","        )\n","\n","    def __len__(self):\n","        return len(self.memory)\n","\n","# Define the Double DQN agent\n","class DoubleDQNAgent:\n","    def __init__(self, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.qnetwork_local = QNetwork(state_size, action_size)\n","        self.qnetwork_target = QNetwork(state_size, action_size)\n","        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=0.001)\n","        self.criterion = nn.MSELoss()\n","        self.replay_buffer = ReplayBuffer(buffer_size=10000, batch_size=64)\n","        self.gamma = 0.99\n","        self.tau = 1e-3\n","        self.update_every = 4\n","        self.t_step = 0\n","\n","    def step(self, state, action, reward, next_state, done):\n","        self.replay_buffer.add(state, action, reward, next_state, done)\n","        self.t_step = (self.t_step + 1) % self.update_every\n","        if self.t_step == 0 and len(self.replay_buffer) > self.replay_buffer.batch_size:\n","            experiences = self.replay_buffer.sample()\n","            self.learn(experiences)\n","\n","    def act(self, state, eps=0.):\n","        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n","        self.qnetwork_local.eval()\n","        with torch.no_grad():\n","            action_values = self.qnetwork_local(state)\n","        self.qnetwork_local.train()\n","        if random.random() > eps:\n","            return np.argmax(action_values.cpu().data.numpy())\n","        else:\n","            return random.choice(np.arange(self.action_size))\n","\n","    def learn(self, experiences):\n","        states, actions, rewards, next_states, dones = experiences\n","\n","        # Double DQN update\n","        Q_local_next = self.qnetwork_local(next_states).detach()\n","        best_actions = torch.argmax(Q_local_next, dim=1).unsqueeze(1)\n","        Q_targets_next = self.qnetwork_target(next_states).detach().gather(1, best_actions)\n","        Q_targets = rewards.unsqueeze(1) + (self.gamma * Q_targets_next * (1 - dones.unsqueeze(1)))\n","\n","        Q_expected = self.qnetwork_local(states).gather(1, actions.unsqueeze(1))\n","\n","        loss = self.criterion(Q_expected, Q_targets)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        self.soft_update(self.qnetwork_local, self.qnetwork_target)\n","\n","    def soft_update(self, local_model, target_model):\n","        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n","            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n","\n","# Create the environment\n","env = gym.make('CartPole-v1')\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n\n","agent = DoubleDQNAgent(state_size, action_size)\n","\n","# Train the agent\n","n_episodes = 1000\n","max_t = 200\n","eps_start = 1.0\n","eps_end = 0.01\n","eps_decay = 0.995\n","rewards_list = []\n","\n","for i_episode in range(1, n_episodes + 1):\n","    state = env.reset()\n","    total_reward = 0\n","    for t in range(max_t):\n","        action = agent.act(state, eps_start)\n","        next_state, reward, done, _ = env.step(action)\n","        agent.step(state, action, reward, next_state, done)\n","        state = next_state\n","        total_reward += reward\n","        if done:\n","            break\n","    eps_start = max(eps_end, eps_decay * eps_start)\n","    rewards_list.append(total_reward)\n","    if i_episode % 100 == 0:\n","        print(f\"Episode {i_episode}/{n_episodes}, Average Reward: {np.mean(rewards_list[-100:])}\")\n","\n","# Plot the rewards\n","plt.plot(rewards_list)\n","plt.xlabel('Episode')\n","plt.ylabel('Total Reward')\n","plt.title('Training Progress')\n","plt.show()\n","\n","# Visualize some episodes\n","for i in range(3):\n","    state = env.reset()\n","    total_reward = 0\n","    done = False\n","    while not done:\n","        action = agent.act(state)\n","        state, reward, done, _ = env.step(action)\n","        total_reward += reward\n","        env.render()\n","    print(f\"Episode {i+1}, Total Reward: {total_reward}\")\n","\n","env.close()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B0Bg2-mQE3DV","outputId":"837330a6-1d67-47dc-da0a-cb8482ab77bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","<ipython-input-2-b980f1bc39ef>:36: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n","  torch.tensor(states, dtype=torch.float32),\n"]},{"output_type":"stream","name":"stdout","text":["Episode 100/1000, Average Reward: 20.32\n","Episode 200/1000, Average Reward: 16.28\n","Episode 300/1000, Average Reward: 13.96\n"]}]},{"cell_type":"markdown","source":["# Initialize replay memory D with capacity N\n","D = ReplayMemory(capacity=N)\n","\n","# Initialize Q-network with random weights\n","Q = NeuralNetwork()\n","\n","# Initialize target Q-network with the same weights as Q\n","Q_target = NeuralNetwork()\n","\n","# Initialize exploration rate\n","epsilon = 1.0\n","\n","# Main training loop\n","for episode in range(num_episodes):\n","    # Initialize state\n","    state = env.reset()\n","    \n","    # Initialize episode-specific variables\n","    total_reward = 0\n","    done = False\n","    \n","    # While episode is not done\n","    while not done:\n","        # Epsilon-greedy action selection\n","        if random() < epsilon:\n","            action = env.action_space.sample()  # Explore: choose random action\n","        else:\n","            # Double Q-learning action selection\n","            with torch.no_grad():\n","                best_action = argmax(Q(state))\n","                action = best_action\n","        \n","        # Execute action in environment\n","        next_state, reward, done, _ = env.step(action)\n","        \n","        # Store transition (state, action, reward, next_state, done) in replay memory\n","        D.push(state, action, reward, next_state, done)\n","        \n","        # Update total reward\n","        total_reward += reward\n","        \n","        # Sample random minibatch of transitions from replay memory\n","        minibatch = D.sample_batch(batch_size)\n","        \n","        # Compute Q-values using Q-network\n","        states, actions, rewards, next_states, dones = zip(*minibatch)\n","        \n","        # Compute Q-values for actions using Q-network\n","        with torch.no_grad():\n","            next_actions = argmax(Q(next_states), axis=1)\n","            Q_next = Q_target(next_states)\n","            Q_targets = rewards + gamma * Q_next[range(batch_size), next_actions] * (1 - dones)\n","        \n","        # Update Q-network using gradient descent\n","        Q.update(states, actions, Q_targets)\n","        \n","        # Every C steps, update target Q-network\n","        if steps % C == 0:\n","            Q_target.weights = Q.weights\n","        \n","        # Move to next state\n","        state = next_state\n","        \n","    # Decay exploration rate epsilon\n","    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n","    \n","    # Print episode statistics\n","    print(f\"Episode {episode}: Total Reward = {total_reward}\")\n"],"metadata":{"id":"sPKFpr9t-JJv"}}]}