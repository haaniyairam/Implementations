{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMa4Daw1rEaI67qWBHnrzz5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"S3pYow_cisyS","executionInfo":{"status":"ok","timestamp":1726821022096,"user_tz":-330,"elapsed":575,"user":{"displayName":"Haaniya Iram","userId":"01967222098810418992"}}},"outputs":[],"source":["class MDP:\n","    def __init__(self):\n","        self.states = ['s0', 's1', 's2']\n","        self.actions = ['a0', 'a1']\n","        self.transitions = {\n","            's0': {'a0': [('s0', 0.5), ('s1', 0.5)], 'a1': [('s1', 1.0)]},\n","            's1': {'a0': [('s0', 0.7), ('s2', 0.3)], 'a1': [('s2', 1.0)]},\n","            's2': {'a0': [('s1', 1.0)], 'a1': [('s0', 1.0)]}\n","        }\n","        self.rewards = {\n","            ('s0', 'a0', 's0'): 1,\n","            ('s0', 'a0', 's1'): 2,\n","            ('s0', 'a1', 's1'): 5,\n","            ('s1', 'a0', 's0'): -1,\n","            ('s1', 'a0', 's2'): 3,\n","            ('s1', 'a1', 's2'): 4,\n","            ('s2', 'a0', 's1'): 2,\n","            ('s2', 'a1', 's0'): 0,\n","        }\n","\n","    def transition_probability(self, current_state, action, next_state):\n","        if current_state in self.transitions and action in self.transitions[current_state]:\n","            for next_state_tuple, prob in self.transitions[current_state][action]:\n","                if next_state_tuple == next_state:\n","                    return prob\n","        return 0\n","\n","    def reward(self, current_state, action, next_state):\n","        return self.rewards.get((current_state, action, next_state), 0)\n"]},{"cell_type":"code","source":["import numpy as np\n","\n","class ValueIteration:\n","    def __init__(self, mdp, gamma=0.9, theta=0.0001):\n","        self.mdp = mdp\n","        self.gamma = gamma\n","        self.theta = theta\n","        self.V = {state: 0 for state in self.mdp.states}  # Initialize V(s) = 0 for all states\n","\n","    def run(self):\n","        while True:\n","            delta = 0\n","            for state in self.mdp.states:\n","                v = self.V[state]\n","                # Bellman update\n","                self.V[state] = max(self.calculate_action_value(state, action) for action in self.mdp.actions)\n","                delta = max(delta, abs(v - self.V[state]))\n","            # Stop when the values converge\n","            if delta < self.theta:\n","                break\n","        return self.V\n","\n","    def calculate_action_value(self, state, action):\n","        action_value = 0\n","        for next_state_tuple in self.mdp.transitions[state][action]:\n","            next_state, prob = next_state_tuple\n","            reward = self.mdp.reward(state, action, next_state)\n","            action_value += prob * (reward + self.gamma * self.V[next_state])\n","        return action_value\n","\n","    def derive_policy(self):\n","        policy = {}\n","        for state in self.mdp.states:\n","            best_action = None\n","            best_value = float('-inf')\n","            for action in self.mdp.actions:\n","                action_value = self.calculate_action_value(state, action)\n","                if action_value > best_value:\n","                    best_value = action_value\n","                    best_action = action\n","            policy[state] = best_action\n","        return policy\n"],"metadata":{"id":"cqqnjvFMi4wn","executionInfo":{"status":"ok","timestamp":1726821030334,"user_tz":-330,"elapsed":576,"user":{"displayName":"Haaniya Iram","userId":"01967222098810418992"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Create an instance of the MDP\n","mdp = MDP()\n","\n","# Create an instance of the Value Iteration algorithm\n","value_iteration = ValueIteration(mdp)\n","\n","# Run value iteration to find the optimal value function\n","optimal_values = value_iteration.run()\n","print(\"Optimal Value Function:\")\n","for state, value in optimal_values.items():\n","    print(f\"V({state}) = {value:.2f}\")\n","\n","# Derive the optimal policy\n","optimal_policy = value_iteration.derive_policy()\n","print(\"\\nOptimal Policy:\")\n","for state, action in optimal_policy.items():\n","    print(f\"π({state}) = {action}\")\n"],"metadata":{"id":"0m-65jSbjfBO","executionInfo":{"status":"ok","timestamp":1726821038796,"user_tz":-330,"elapsed":582,"user":{"displayName":"Haaniya Iram","userId":"01967222098810418992"}},"outputId":"a00c398f-fc24-482a-8eed-6a0f915f3284","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Value Function:\n","V(s0) = 32.47\n","V(s1) = 30.53\n","V(s2) = 29.47\n","\n","Optimal Policy:\n","π(s0) = a1\n","π(s1) = a1\n","π(s2) = a0\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","class ValueIterationWithTracking(ValueIteration):\n","    def __init__(self, mdp, gamma=0.9, theta=0.0001):\n","        super().__init__(mdp, gamma, theta)\n","        self.history = []\n","\n","    def run(self):\n","        while True:\n","            delta = 0\n","            values_snapshot = {}\n","            for state in self.mdp.states:\n","                v = self.V[state]\n","                self.V[state] = max(self.calculate_action_value(state, action) for action in self.mdp.actions)\n","                delta = max(delta, abs(v - self.V[state]))\n","                values_snapshot[state] = self.V[state]\n","            self.history.append(values_snapshot)  # Store the value function for each iteration\n","            if delta < self.theta:\n","                break\n","        return self.V\n","\n","    def plot_value_convergence(self):\n","        plt.figure()\n","        iterations = range(len(self.history))\n","        for state in self.mdp.states:\n","            values = [v[state] for v in self.history]\n","            plt.plot(iterations, values, label=f'V({state})')\n","        plt.xlabel('Iterations')\n","        plt.ylabel('Value')\n","        plt.title('Value Function Convergence')\n","        plt.legend()\n","        plt.show()\n","\n","# Create an instance of the Value Iteration with tracking\n","value_iteration_tracking = ValueIterationWithTracking(mdp)\n","\n","# Run value iteration\n","value_iteration_tracking.run()\n","\n","# Plot the value function convergence\n","value_iteration_tracking.plot_value_convergence()\n"],"metadata":{"id":"joc50d-njhGA"},"execution_count":null,"outputs":[]}]}