{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOk4rhkQCIivY1Qf/7C4VhT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"hwS2b9NoUjey","executionInfo":{"status":"ok","timestamp":1726817231595,"user_tz":-330,"elapsed":467,"user":{"displayName":"Haaniya Iram","userId":"01967222098810418992"}}},"outputs":[],"source":["class MDP:\n","    def __init__(self):\n","        self.states = ['s0', 's1']\n","        self.actions = ['a0', 'a1']\n","        self.transitions = {\n","            's0': {'a0': [('s0', 0.7), ('s1', 0.3)]},\n","            's1': {'a1': [('s0', 0.4), ('s1', 0.6)]}\n","        }\n","        self.rewards = {\n","            ('s0', 'a0', 's0'): 1,\n","            ('s0', 'a0', 's1'): 0,\n","            ('s1', 'a1', 's0'): 2,\n","            ('s1', 'a1', 's1'): 3,\n","        }\n","\n","    def transition_probability(self, current_state, action, next_state):\n","        if current_state in self.transitions and action in self.transitions[current_state]:\n","            for next_state_tuple, prob in self.transitions[current_state][action]:\n","                if next_state_tuple == next_state:\n","                    return prob\n","        return 0\n"]},{"cell_type":"code","source":["class MDPWithValueFunction(MDP):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def value_function(self, state, policy, discount_factor=0.9, num_steps=100):\n","        value = 0\n","        for step in range(num_steps):\n","            for action in policy[state]:\n","                # Sum over possible next states\n","                for next_state_tuple in self.transitions[state][action]:\n","                    next_state, prob = next_state_tuple\n","                    reward = self.rewards.get((state, action, next_state), 0)\n","                    value += prob * (reward + discount_factor * value)\n","        return value\n"],"metadata":{"id":"_8yMggsGUzpW","executionInfo":{"status":"ok","timestamp":1726817239691,"user_tz":-330,"elapsed":600,"user":{"displayName":"Haaniya Iram","userId":"01967222098810418992"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Create MDP instance with action value function calculation\n","mdp_action_value_function = MDPWithActionValueFunction()\n","\n","# Example policy definition\n","policy = {\n","    's0': ['a0'],\n","    's1': ['a1'],\n","}\n","\n","# Example usage\n","state = 's0'\n","action = 'a0'\n","q_value = mdp_action_value_function.action_value_function(state, action, policy)\n","print(f\"Action-Value Function qπ({state}, {action}) = {q_value}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AU3E_iXyVKzS","executionInfo":{"status":"ok","timestamp":1726817288709,"user_tz":-330,"elapsed":948,"user":{"displayName":"Haaniya Iram","userId":"01967222098810418992"}},"outputId":"e1a4db0f-03b6-4311-ff82-9e7def40f638"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Action-Value Function qπ(s0, a0) = 1.1626742605649279e+34\n"]}]}]}